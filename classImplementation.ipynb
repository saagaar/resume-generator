{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c4c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda1318",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\n  \"cv_data\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 454\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# Start interactive loop\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     show_question(ask_model())\n\u001b[0;32m--> 454\u001b[0m \u001b[43mapplicantOverview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 451\u001b[0m, in \u001b[0;36mapplicantOverview\u001b[0;34m(cv_file, suggestion_file, jd_file)\u001b[0m\n\u001b[1;32m    448\u001b[0m     display(text_box, submit_button)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Start interactive loop\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m show_question(\u001b[43mask_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[2], line 409\u001b[0m, in \u001b[0;36mapplicantOverview.<locals>.ask_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    403\u001b[0m finalprompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m    404\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt),\n\u001b[1;32m    405\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, human_prompt)\n\u001b[1;32m    406\u001b[0m ])\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# ✅ Use the structured prompt, not just the string\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[43mfinalprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:1188\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1187\u001b[0m ):\n\u001b[0;32m-> 1188\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:559\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m        List of BaseMessages.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:592\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    Formatted message.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt, StringPromptTemplate):\n\u001b[0;32m--> 592\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_msg_class(\n\u001b[1;32m    594\u001b[0m         content\u001b[38;5;241m=\u001b[39mtext, additional_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs\n\u001b[1;32m    595\u001b[0m     )\n\u001b[1;32m    596\u001b[0m content: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/prompt.py:187\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    A formatted string.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:161\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/utils/formatting.py:33\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:165\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[1;32m    164\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 165\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:205\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    201\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:270\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[1;32m    268\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 270\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:227\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\n  \"cv_data\"'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from file_handler import extract_text\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "def convertCVToJson() -> None:\n",
    "  api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "  # api_base = os.getenv(\"OPENROUTER_API_BASE\", \"https://openrouter.ai/api/v1\")\n",
    "  # model = os.getenv(\"OPENROUTER_MODEL\", \"meta-llama/llama-3-8b-instruct\")\n",
    "  # Following steps to workout tomoorow\n",
    "  #1. Use ipynb to upload a file and save it to a destination\n",
    "  #2. Extract the content from pdf or docx\n",
    "  # 3. Convert it to json using LLM\n",
    "  # 4. Save the json to a file\n",
    "  # 5. use the content of json to rewrite the cover letter \n",
    "  resume_text = extract_text(\"cv/my_cv.pdf\")\n",
    "  # client = OpenAI()\n",
    "  from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "  schema = {\n",
    "    \"personal_information\": {\n",
    "      \"name\": \"\",\n",
    "      \"email\": \"\",\n",
    "      \"phone\": \"\",\n",
    "      \"location\": \"\"\n",
    "    },\n",
    "    \"summary\": \"\",\n",
    "    \"skills\": [],\n",
    "    \"experience\": [\n",
    "      {\n",
    "        \"role\": \"\",\n",
    "        \"company\": \"\",\n",
    "        \"start_date\": \"\",\n",
    "        \"end_date\": \"\",\n",
    "        \"description\": []\n",
    "      }\n",
    "    ],\n",
    "    \"education\": [\n",
    "      {\n",
    "        \"degree\": \"\",\n",
    "        \"institution\": \"\",\n",
    "        \"year\": \"\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  # ⚙️ Initialize OpenRouter model (via OpenAI-compatible client)\n",
    "  llm = ChatOpenAI(\n",
    "      model=\"openai/gpt-4o-mini\",   # can swap with other OpenRouter models\n",
    "      api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "      base_url=\"https://openrouter.ai/api/v1\",\n",
    "      temperature=0\n",
    "  )\n",
    "\n",
    "  # 📝 Prompt setup\n",
    "  system = \"You are a strict JSON extractor.\"\n",
    "  human_prompt = \"\"\"\n",
    "  Extract the following CV into this JSON schema: {schema_content}\n",
    "\n",
    "  Rules:\n",
    "  - Copy content exactly into the schema.\n",
    "  - Do not invent anything. Leave blank if missing.\n",
    "  - Only return valid JSON.\n",
    "\n",
    "  CV TEXT:\n",
    "  {resume_text}\n",
    "  \"\"\"\n",
    "  prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system),\n",
    "      (\"human\", human_prompt)\n",
    "  ])\n",
    "  chain = prompt | llm\n",
    "\n",
    "  # Run chain\n",
    "  result = chain.invoke({\"resume_text\": resume_text,\"schema_content\": json.dumps(schema, indent=2)})\n",
    "\n",
    "  # Parse model output\n",
    "  try:\n",
    "      if(result.content):\n",
    "        extracted_json = json.loads(result.content)\n",
    "        filename='cv/cv.json';\n",
    "        print(\"✅ Extracted JSON:\")\n",
    "        print(json.dumps(extracted_json, indent=2)) \n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(extracted_json, file, indent=2)\n",
    "      else:\n",
    "         print  (\"❌ No content returned from model\") \n",
    "      \n",
    "  except json.JSONDecodeError:\n",
    "      print(\"❌ Model returned invalid JSON:\")\n",
    "      print(result.content)\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def suggestionsOnSkillSetJson():\n",
    "  jd=(read_file('cv/jd.txt'))\n",
    "  skillset= {\n",
    "                \"resume_improvements\": \"string - general advice on content, structure, tone, formatting\",\n",
    "                \"skills_required\": [\"string\", \"string\", ...],\n",
    "                \"experience_examples\": [\"string\", \"string\", ...],\n",
    "                \"soft_skills\": [\"string\", \"string\", ...],\n",
    "                \"certifications\": [\"string\", \"string\", ...],\n",
    "                \"keywords_for_ATS\": [\"string\", \"string\", ...]\n",
    "              }\n",
    "  system = \"\"\"You are a strict JSON extractor.\n",
    "              Always return output as **valid JSON only** following this schema: \n",
    "              {skillset}\n",
    "            - Replace generic terms with industry-specific language.\n",
    "            - Keep sentences short and natural with transitional phrases, like a human career coach speaking directly.       \n",
    "            - Never include extra commentary outside the JSON.\n",
    "            - Never use Markdown, code blocks, or ```json fencing.\n",
    "            - Output raw JSON only.\n",
    "            \"\"\"\n",
    "  human_prompt=\"\"\"\n",
    "                You are a professional career coach with decades of HR and recruitment experience. \n",
    "                I want to apply for the following position:\n",
    "                {jd}\n",
    "              Please provide me with up-to-date information on the skills and qualifications usually required for this role,\n",
    "                examples of prior experience or work experience that would make me an exceptional candidate during a recruitment process for this role, \n",
    "              information on the soft skills, aptitudes and personality traits that employers are likely to recognize as valuable in this role, and suggestions for recognized certifications or training that would improve my chances of success \n",
    "              and \n",
    "              Suggest 5 keywords I should add to my resume to improve ATS compatibility in JSon format\n",
    "  \"\"\"\n",
    "  llm = ChatGoogleGenerativeAI(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      temperature=0,\n",
    "      max_tokens=None,\n",
    "      timeout=None,\n",
    "      max_retries=2,\n",
    "  )\n",
    "  finalprompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system),\n",
    "      (\"human\", human_prompt)\n",
    "  ])\n",
    "  chain = finalprompt | llm\n",
    "\n",
    "  # Run chain\n",
    "  result = chain.invoke({\"jd\": jd,\"skillset\":skillset})\n",
    "  print(result)\n",
    "  content = result.content.strip()\n",
    "  try:\n",
    "        suggestions = json.loads(content)\n",
    "        filename='cv/suggestions.json';\n",
    "        print(suggestions)\n",
    "        print(json.dumps(suggestions, indent=2)) \n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(suggestions, file, indent=2)\n",
    "  except json.JSONDecodeError:\n",
    "      print(json.JSONDecodeError)\n",
    "      print(\"❌ Model returned invalid JSON:\")\n",
    "      # print(result.content)\n",
    "\n",
    "\n",
    "# def applicantOverview(cv_file=\"cv/cv.json\", suggestion_file=\"cv/suggestions.json\", jd_file=\"cv/jd.txt\"):\n",
    "#     \"\"\"\n",
    "#     Generate a tailored personal statement for a job application\n",
    "#     using candidate CV, recruiter suggestions, and job description.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load CV JSON\n",
    "#     with open(cv_file, \"r\") as f:\n",
    "#         cv_data = json.load(f)\n",
    "\n",
    "#     # Load suggestion JSON\n",
    "#     with open(suggestion_file, \"r\") as f:\n",
    "#         suggestion_data = json.load(f)\n",
    "\n",
    "#     # Load job description text\n",
    "#     with open(jd_file, \"r\") as f:\n",
    "#         jd_text = f.read().strip()\n",
    "\n",
    "#     # Build system instructions\n",
    "#     system = \"\"\"You are an expert resume analyst, career advisor, and HR professional specializing in the tech industry.\n",
    "# You have access to:\n",
    "# - cv.json (candidate skills, qualifications, and work history)\n",
    "# - suggestion.json (role-specific recruiter advice, skills, examples, certifications, ATS keywords)\n",
    "# - jd.txt (job description of the target role)\n",
    "\n",
    "# Instructions:\n",
    "# 1. Confirm you have processed the provided files.\n",
    "# 2. Ask the candidate clarifying questions one at a time to understand why they are the perfect fit.\n",
    "# 3. When you have enough information, generate a polished personal statement of up to 150 words:\n",
    "#    - Professional yet natural tone, not robotic.\n",
    "#    - Concise, attention-grabbing, and tailored to the job.\n",
    "#    - Showcase achievements, technical skills, and impact.\n",
    "#    - Seamlessly include ATS keywords from suggestion.json and relevant terms from jd.txt.\n",
    "#    - Align tone with the company’s values and role requirements.\n",
    "# Do not create the statement until you have asked the essential questions you need.\n",
    "# \"\"\"\n",
    "\n",
    "#     human_template = \"\"\"\n",
    "# Candidate CV:\n",
    "# {cv_data}\n",
    "\n",
    "# Recruiter Suggestions:\n",
    "# {suggestion_data}\n",
    "\n",
    "# Job Description:\n",
    "# {jd_text}\n",
    "# \"\"\"\n",
    "\n",
    "#     # Create prompt\n",
    "#     finalprompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", human_template)\n",
    "#     ])\n",
    "\n",
    "#     # Initialize model\n",
    "#     llm = ChatGoogleGenerativeAI(\n",
    "#         model=\"gemini-2.5-flash\",\n",
    "#         temperature=0.3,\n",
    "#         max_tokens=None,\n",
    "#         timeout=None,\n",
    "#         max_retries=2,\n",
    "#     )\n",
    "\n",
    "#     chain = finalprompt | llm\n",
    "\n",
    "#     # Run the chain\n",
    "#     result = chain.invoke({\n",
    "#         \"cv_data\": json.dumps(cv_data, indent=2),\n",
    "#         \"suggestion_data\": json.dumps(suggestion_data, indent=2),\n",
    "#         \"jd_text\": jd_text\n",
    "#     })\n",
    "#     content = result.content.strip()\n",
    "#     while True:\n",
    "#         print(\"\\n🤖 Assistant:\", content)\n",
    "\n",
    "#         # If model produced the final statement → stop\n",
    "#         if \"FINAL_STATEMENT:\" in content:\n",
    "#             break\n",
    "\n",
    "#         display(\"Your Answer: \")\n",
    "#         user_input = input()\n",
    "#         # Otherwise, wait for your input\n",
    "#         user_input = input(\"\\n✍️ Your Answer: \")\n",
    "\n",
    "#         # Send answer back into conversation\n",
    "#         content = llm.invoke(user_input).content\n",
    "#         return result.content\n",
    "\n",
    "# def applicantOverview(cv_file=\"cv/cv.json\", suggestion_file=\"cv/suggestions.json\", jd_file=\"cv/jd.txt\"):\n",
    "#     # Load candidate files\n",
    "#     with open(cv_file, \"r\") as f:\n",
    "#         cv_data = json.load(f)\n",
    "#     with open(suggestion_file, \"r\") as f:\n",
    "#         suggestion_data = json.load(f)\n",
    "#     with open(jd_file, \"r\") as f:\n",
    "#         jd_text = f.read().strip()\n",
    "\n",
    "#     # Conversation history\n",
    "#     conversation_history = []\n",
    "\n",
    "#     # Initialize LLM\n",
    "#     llm = ChatGoogleGenerativeAI(\n",
    "#         model=\"gemini-2.5-flash\",\n",
    "#         temperature=0.3,\n",
    "#         max_tokens=None,\n",
    "#         timeout=None,\n",
    "#         max_retries=2,\n",
    "#     )\n",
    "\n",
    "#     # System prompt for the LLM\n",
    "#     system_prompt = \"\"\"You are an expert resume analyst and HR professional in tech.\n",
    "# You have access to:\n",
    "# - Candidate CV (skills, achievements)\n",
    "# - Recruiter suggestions (role-specific advice, ATS keywords)\n",
    "# - Job description\n",
    "# Task:\n",
    "# 1. Ask the candidate clarifying questions one at a time to collect information for a personal statement.\n",
    "# 2. Only ask questions you need to generate a polished 150-word personal statement.\n",
    "# 3. Wait for candidate’s answer before asking the next question.\n",
    "# 4. Once you have enough info, output the final statement starting with \"FINAL_STATEMENT:\".\n",
    "# 5. Include relevant ATS keywords. Use a professional but human tone.\n",
    "# \"\"\"\n",
    "\n",
    "#     # Function to ask the model for next question or final statement\n",
    "#     def ask_model():\n",
    "#         # Build prompt with embedded JSON and conversation history\n",
    "#         history_text = \"\"\n",
    "#         for i, msg in enumerate(conversation_history):\n",
    "#             history_text += f\"Q{i+1}: {msg['question']}\\nA{i+1}: {msg['answer']}\\n\"\n",
    "\n",
    "#         human_prompt = f\"\"\"\n",
    "# Candidate CV:\n",
    "# {json.dumps(cv_data, indent=2)}\n",
    "\n",
    "# Recruiter Suggestions:\n",
    "# {json.dumps(suggestion_data, indent=2)}\n",
    "\n",
    "# Job Description:\n",
    "# {jd_text}\n",
    "\n",
    "# Conversation so far:\n",
    "# {history_text}\n",
    "\n",
    "# Please generate the next question for the candidate, or output FINAL_STATEMENT: if you have enough information.\n",
    "# \"\"\"\n",
    "#         finalprompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", system_prompt),\n",
    "#             (\"human\", human_prompt)\n",
    "#         ])\n",
    "\n",
    "#         response = llm.invoke(human_prompt)  # human_prompt is a string\n",
    "#         return response.content\n",
    "\n",
    "#     # Function to display question and text box\n",
    "#     def show_question(response_text):\n",
    "#         clear_output()\n",
    "#         if response_text.startswith(\"FINAL_STATEMENT:\"):\n",
    "#             # Print final personal statement\n",
    "#             print(\"📝 Personal Statement Generated:\\n\")\n",
    "#             print(response_text.replace(\"FINAL_STATEMENT:\", \"\").strip())\n",
    "#             return\n",
    "\n",
    "#         # Show question to user\n",
    "#         print(\"🤖\", response_text)\n",
    "#         text_box = widgets.Textarea(\n",
    "#             placeholder='Type your answer here...',\n",
    "#             layout=widgets.Layout(width='100%', height='100px')\n",
    "#         )\n",
    "#         submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "#         def on_submit(b):\n",
    "#             # Save answer\n",
    "#             print('button is submitted')\n",
    "#             conversation_history.append({\"question\": response_text, \"answer\": text_box.value})\n",
    "#             # Ask next question\n",
    "#             show_question(ask_model())\n",
    "\n",
    "#         submit_button.on_click(on_submit)\n",
    "#         display(text_box, submit_button)\n",
    "\n",
    "#     # Start the interactive loop\n",
    "#     show_question(ask_model())\n",
    "# applicantOverview();\n",
    "\n",
    "def applicantOverview(cv_file=\"cv/cv.json\", suggestion_file=\"cv/suggestions.json\", jd_file=\"cv/jd.txt\"):\n",
    "    # Conversation history (Q&A)\n",
    "    conversation_history = []\n",
    "    with open(cv_file, \"r\") as f:\n",
    "        cv_data = json.load(f)\n",
    "    with open(suggestion_file, \"r\") as f:\n",
    "        suggestion_data = json.load(f)\n",
    "    with open(jd_file, \"r\") as f:\n",
    "        jd_text = f.read().strip()\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    # System prompt for the LLM\n",
    "    system_prompt =\"\"\"You are an expert resume analyst and HR professional in tech.\n",
    "    You have access to:\n",
    "    - Candidate CV (skills, achievements)\n",
    "    - Recruiter suggestions (role-specific advice, ATS keywords)\n",
    "    - Job description\n",
    "    Task:\n",
    "    1. Ask the candidate clarifying questions one at a time to collect information for a personal statement.\n",
    "    2. Only ask questions you need to generate a polished 150-word personal statement.\n",
    "    3. Wait for candidate’s answer before asking the next question.\n",
    "    4. Once you have enough info, output the final statement starting with \"FINAL_STATEMENT:\".\n",
    "    5. Include relevant ATS keywords. Use a professional but human tone.\n",
    "    \"\"\"\n",
    "\n",
    "    def ask_model():\n",
    "        # Merge everything into one context\n",
    "        merged_context = {\n",
    "            \"cv_data\": cv_data,\n",
    "            \"suggestions\": suggestion_data,\n",
    "            \"job_description\": jd_text,\n",
    "            \"qna\": conversation_history\n",
    "        }\n",
    "        human_prompt = f\"\"\"\n",
    "        Here is all available information about the candidate:\n",
    "        {json.dumps(merged_context, indent=2)}\n",
    "        Please generate the next clarifying question for the candidate, or output FINAL_STATEMENT: if you have enough information.\n",
    "        \"\"\"\n",
    "        finalprompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", human_prompt)\n",
    "        ])\n",
    "\n",
    "        # ✅ Use the structured prompt, not just the string\n",
    "        response = llm.invoke(finalprompt.format_messages())\n",
    "        return response.content\n",
    "\n",
    "    def show_question(response_text):\n",
    "        clear_output()\n",
    "        if response_text.startswith(\"FINAL_STATEMENT:\"):\n",
    "            # Print final personal statement\n",
    "            print(\"📝 Personal Statement Generated:\\n\")\n",
    "            print(response_text.replace(\"FINAL_STATEMENT:\", \"\").strip())\n",
    "\n",
    "            # Save combined context\n",
    "            final_context = {\n",
    "                \"cv_data\": cv_data,\n",
    "                \"suggestions\": suggestion_data,\n",
    "                \"job_description\": jd_text,\n",
    "                \"qna\": conversation_history,\n",
    "                \"final_statement\": response_text.replace(\"FINAL_STATEMENT:\", \"\").strip()\n",
    "            }\n",
    "            with open(\"application_context.json\", \"w\") as f:\n",
    "                json.dump(final_context, f, indent=2)\n",
    "\n",
    "            print(\"\\n✅ Saved in application_context.json\")\n",
    "            return\n",
    "\n",
    "        # Show question\n",
    "        print(\"🤖\", response_text)\n",
    "        text_box = widgets.Textarea(\n",
    "            placeholder='Type your answer here...',\n",
    "            layout=widgets.Layout(width='100%', height='100px')\n",
    "        )\n",
    "        submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "        def on_submit(b):\n",
    "            # Save Q&A\n",
    "            conversation_history.append({\"question\": response_text, \"answer\": text_box.value})\n",
    "            # Ask next question\n",
    "            show_question(ask_model())\n",
    "\n",
    "        submit_button.on_click(on_submit)\n",
    "        display(text_box, submit_button)\n",
    "\n",
    "    # Start interactive loop\n",
    "    show_question(ask_model())\n",
    "\n",
    "\n",
    "applicantOverview()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d8ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 CV JSON Keys: ['personal_information', 'summary', 'skills', 'experience', 'education']\n",
      "📂 Suggestions JSON Keys: ['resume_improvements', 'skills_required', 'experience_examples', 'soft_skills', 'certifications', 'keywords_for_ATS']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\\n  \"personal_information\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 125\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_question(first_question)\n\u001b[1;32m    124\u001b[0m app \u001b[38;5;241m=\u001b[39m ApplicantOverview()\n\u001b[0;32m--> 125\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 120\u001b[0m, in \u001b[0;36mApplicantOverview.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Begin the interactive Q&A loop.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     first_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_question(first_question)\n",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m, in \u001b[0;36mApplicantOverview.ask_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         human_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124mCandidate CV:\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_data,\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124mPlease generate the next question for the candidate, or output FINAL_STATEMENT: if you have enough information.\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         finalprompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m     86\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt),\n\u001b[1;32m     87\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, human_prompt)\n\u001b[1;32m     88\u001b[0m         ])\n\u001b[0;32m---> 90\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke(\u001b[43mfinalprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuman_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuman_prompt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:703\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    694\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the chat template into a string.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m        formatted string.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_string()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:726\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptValue:\n\u001b[1;32m    718\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a ChatPromptValue.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m        ChatPromptValue.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:1188\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1187\u001b[0m ):\n\u001b[0;32m-> 1188\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:559\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m        List of BaseMessages.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/chat.py:592\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    Formatted message.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt, StringPromptTemplate):\n\u001b[0;32m--> 592\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_msg_class(\n\u001b[1;32m    594\u001b[0m         content\u001b[38;5;241m=\u001b[39mtext, additional_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs\n\u001b[1;32m    595\u001b[0m     )\n\u001b[1;32m    596\u001b[0m content: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/prompts/prompt.py:187\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    A formatted string.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:161\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/langchain_core/utils/formatting.py:33\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:165\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[1;32m    164\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 165\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:205\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    201\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:270\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[1;32m    268\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 270\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/string.py:227\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\n  \"personal_information\"'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyADKXE3BLwVOx6ERtZ9l4Uk_NXHTVb9caA\"\n",
    "class ApplicantOverview:\n",
    "    def __init__(self, cv_file=\"cv/cv.json\", suggestion_file=\"cv/suggestions.json\", jd_file=\"cv/jd.txt\"):\n",
    "        def safe_load_json(path):\n",
    "            try:\n",
    "                with open(path, \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {path}\")\n",
    "                return {}\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Invalid JSON in {path}: {e}\")\n",
    "                return {}\n",
    "\n",
    "        self.cv_data = safe_load_json(cv_file)\n",
    "        self.suggestion_data = safe_load_json(suggestion_file)\n",
    "\n",
    "        try:\n",
    "            with open(jd_file, \"r\") as f:\n",
    "                self.jd_text = f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️ File not found: {jd_file}\")\n",
    "            self.jd_text = \"\"\n",
    "        print(\"📂 CV JSON Keys:\", list(self.cv_data.keys()))\n",
    "        print(\"📂 Suggestions JSON Keys:\", list(self.suggestion_data.keys()))\n",
    "\n",
    "        # Conversation history\n",
    "        \n",
    "        self.conversation_history = []\n",
    "  \n",
    "       \n",
    "  # ⚙️ Initialize OpenRouter model (via OpenAI-compatible client)\n",
    "  \n",
    "        # LLM\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "\n",
    "        # System prompt\n",
    "        self.system_prompt =\"\"\"You are an expert resume analyst and HR professional in tech.\n",
    "        You have access to:\n",
    "        - Candidate CV (skills, achievements)\n",
    "        - Recruiter suggestions (role-specific advice, ATS keywords)\n",
    "        - Job description\n",
    "        Task:\n",
    "        1. Ask the candidate clarifying questions one at a time to collect information for a personal statement.\n",
    "        2. Only ask questions you need to generate a polished 150-word personal statement.\n",
    "        3. Wait for candidate’s answer before asking the next question.\n",
    "        4. Once you have enough info, output the final statement starting with \"FINAL_STATEMENT:\".\n",
    "        5. Include relevant ATS keywords. Use a professional but human tone.\n",
    "        \"\"\"\n",
    "\n",
    "    def ask_model(self):\n",
    "        \"\"\"Build prompt with CV, suggestions, JD and conversation history.\"\"\"\n",
    "        history_text = \"\"\n",
    "        for i, msg in enumerate(self.conversation_history):\n",
    "            history_text += f\"Q{i+1}: {msg['question']}\\nA{i+1}: {msg['answer']}\\n\"\n",
    "\n",
    "        human_prompt = f\"\"\"\n",
    "Candidate CV:\n",
    "{json.dumps(self.cv_data, indent=2)}\n",
    "\n",
    "Recruiter Suggestions:\n",
    "{json.dumps(self.suggestion_data, indent=2)}\n",
    "\n",
    "Job Description:\n",
    "{self.jd_text}\n",
    "\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "Please generate the next question for the candidate, or output FINAL_STATEMENT: if you have enough information.\n",
    "\"\"\"\n",
    "\n",
    "        finalprompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system_prompt),\n",
    "            (\"human\", human_prompt)\n",
    "        ])\n",
    "\n",
    "        response = self.llm.invoke(finalprompt.format(human_prompt=human_prompt))\n",
    "        return response.content\n",
    "\n",
    "    def show_question(self, response_text):\n",
    "        \"\"\"Display question, capture answer, and move to next step.\"\"\"\n",
    "        clear_output()\n",
    "        if response_text.startswith(\"FINAL_STATEMENT:\"):\n",
    "            print(\"📝 Personal Statement Generated:\\n\")\n",
    "            print(response_text.replace(\"FINAL_STATEMENT:\", \"\").strip())\n",
    "            return\n",
    "\n",
    "        # Show the question\n",
    "        print(\"🤖\", response_text)\n",
    "        text_box = widgets.Textarea(\n",
    "            placeholder='Type your answer here...',\n",
    "            layout=widgets.Layout(width='100%', height='100px')\n",
    "        )\n",
    "        submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "        def on_submit(b):\n",
    "            # Save answer\n",
    "            self.conversation_history.append({\"question\": response_text, \"answer\": text_box.value})\n",
    "            # Ask next question\n",
    "            print(self.conversation_history)\n",
    "            self.show_question(self.ask_model())\n",
    "        submit_button.on_click(on_submit)\n",
    "        display(text_box, submit_button)\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Begin the interactive Q&A loop.\"\"\"\n",
    "        first_question = self.ask_model()\n",
    "        self.show_question(first_question)\n",
    "\n",
    "\n",
    "app = ApplicantOverview()\n",
    "app.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24409d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
